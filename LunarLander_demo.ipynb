{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72996bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "from ppo import PPO\n",
    "from test_policy import test_policy, plot_animation\n",
    "from network import FeedForwardNN\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9a067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim, act_dim 8 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print('obs_dim, act_dim', obs_dim, act_dim)\n",
    "\n",
    "\n",
    "actor = FeedForwardNN(obs_dim, act_dim)\n",
    "critic = FeedForwardNN(obs_dim, 1)\n",
    "\n",
    "savename = 'lunarlandercont'\n",
    "actor.load_state_dict(torch.load('../Models/'+savename+'ppo_actor.pth'))\n",
    "critic.load_state_dict(torch.load('../Models/'+savename+'ppo_critic.pth'))\n",
    "\n",
    "model = PPO(env, actor, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776c4c7",
   "metadata": {},
   "source": [
    "# Solved is 200 points\n",
    "\n",
    "https://xaviergeerinck.com/post/ai/rl/openai-lunar-lander\n",
    "\n",
    "The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n",
    "Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n",
    "If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n",
    "comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n",
    "Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n",
    "Solved is 200 points.\n",
    "Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt. Please see the source code for details.\n",
    "\n",
    "stats space is dim 8\n",
    "\n",
    "1,2: first 2 are position in x axis and y axis(hieght) \n",
    "3,4: other 2 are the x,y axis velocity terms, \n",
    "5,6: lander angle and angular velocity, \n",
    "7,8: left and right left contact points\n",
    "\n",
    "Observation Space: The observation space is illustrated by a \"Box\" containing 8 values between [ $-\\infty$, $\\infty$ ] these values are:\n",
    "\n",
    "- Position X\n",
    "- Position Y\n",
    "- Velocity X\n",
    "- Velocity Y\n",
    "- Angle\n",
    "- Angular Velocity\n",
    "- Is left leg touching the ground: 0 OR 1\n",
    "- Is right leg touching the ground: 0 OR 1\n",
    "\n",
    "Action Space:\n",
    "\n",
    "Discrete (Discrete Action Space with 4 values):\n",
    "- 0 = Do Nothing\n",
    "- 1 = Fire Left Engine\n",
    "- 2 = Fire Main Engine\n",
    "- 3 = Fire Right Engine\n",
    "\n",
    "Continuous (Box Action Space with 2 values between -1 and +1):\n",
    "- Value 1: [-1.0, +1.0] for main engine where [-1.0, 0.0] = Off and [0.0, +1.0] = On\n",
    "- Value 2:\n",
    "    -[-1.0, -0.5]: Left Engine\n",
    "    - [-0.5, 0.5]: Off\n",
    "    - [0.5, 1.0]: Right Engine\n",
    "\n",
    "Reward Function:\n",
    "\n",
    "The Reward Function is a bit more complex and consists out of multiple components:\n",
    "\n",
    "- [100, 140] points for Moving to the landing pad and zero speed\n",
    "- Negative reward for moving away from the landing pad\n",
    "- If lander crashes or comes to rest it gets -100 or +100\n",
    "- Each leg with ground contact gets +10\n",
    "- Firing the main engine is -0.3 per frame\n",
    "- Firing the side engine is -0.03 per frame\n",
    "- Solved is 200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1_000_000, track_progress = True, savename = 'lunarlandercont2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46248b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episode: total timesteps 293 total rewards 289.16941026360473\n"
     ]
    }
   ],
   "source": [
    "# Build our policy the same way we build our actor model in PPO\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "env.seed(42) # 247 289.8606570638711\n",
    "\n",
    "#torch.manual_seed(0) \n",
    "#policy = model.actor\n",
    "\n",
    "# Load in the actor model saved by the PPO algorithm\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "actor = FeedForwardNN(obs_dim, act_dim)\n",
    "\n",
    "savename = 'lunarlandercont2'\n",
    "actor.load_state_dict(torch.load('Models/'+savename+'ppo_actor.pth'))\n",
    "critic.load_state_dict(torch.load('Models/'+savename+'ppo_critic.pth'))\n",
    "    \n",
    "episode_len, episode_return, frames = test_policy(actor, env, render = True)\n",
    "print('1 episode: total timesteps', episode_len, 'total rewards', episode_return) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e852335",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c360b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
